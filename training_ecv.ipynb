{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82adac62",
   "metadata": {},
   "source": [
    "# Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt\n",
    "! pip ipykernel --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import plotly.io as pio\n",
    "#from src.utils import is_function, is_operator, is_variable\n",
    "import copy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 30\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# DATA PARAMETERS\n",
    "max_tree_depth = None                                               # maximum depth of binary (equation) trees\n",
    "num_equation_samples = 6000                                        # number of equations to sample\n",
    "max_num_input_variables = 1                                        # maximum number of input variables (x1, x2, ...)\n",
    "max_num_constants = 1                                              # maximum number of constants (c1, c2, ...)\n",
    "num_evaluation_samples = 50                                        # number of points to evaluate the entire equations at\n",
    "inf_repacement = 1000\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "training_set_proportion = 0.8                           # proportion of data to use for training\n",
    "num_epochs = 500                                       # number of epochs to train for\n",
    "batch_size = 50                                         # batch size\n",
    "learning_rate = 0.01\n",
    "record = True\n",
    "kl_weight = 0.001\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "latent_dims = 8\n",
    "\n",
    "is_function = lambda x: x in [\"sin\", \"cos\", \"tan\", \"exp\", \"log\", \"sqrt\", \"abs\", \"acos\",\"asin\", \"arcsin\", \"arccos\"]\n",
    "is_operator = lambda x : x in [\"+\", \"-\", \"*\", \"/\", \"**\", \"max\", \"min\"]\n",
    "is_variable = lambda x : x in [\"x_1\"]\n",
    "is_constant = lambda x : x in [\"c_1\", \"c_0\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "![](./images/dataloader.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.preprocessing import generate_dataset, preprocessing\n",
    "\n",
    "# load or create dataset\n",
    "try: \n",
    "    dataset = torch.load(\"./data/dataset_6000.pt\")\n",
    "except FileNotFoundError:\n",
    "    dataset, max_len, unique_symbols = generate_dataset(num_equation_samples)\n",
    "    torch.save(dataset, \"./data/dataset_6000.pt\")\n",
    "\n",
    "train_loader, test_loader, test_size = preprocessing(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    training_set_proportion=training_set_proportion\n",
    ")\n",
    "\n",
    "equations = [dataset.decode_equation(x[0]) for x in dataset]\n",
    "constants = [list(x[1].detach().numpy()) for x in dataset]\n",
    "values = [list(x[2].detach().numpy()) for x in dataset] \n",
    "all_symbols = [item for sublist in equations for item in sublist]\n",
    "unique_symbols = sorted(list(set(all_symbols)))\n",
    "max_len = len(equations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.evaluation import plot_functions\n",
    "\n",
    "fig = plot_functions(\n",
    "    equations=equations[:20],\n",
    "    constants=constants[:20],\n",
    "    values=values[:20],\n",
    "    is_function=is_function,\n",
    "    is_operator=is_operator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE ECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.training import training_VAE\n",
    "\n",
    "autoencoder_equations, train_losses, test_losses, correlations_cor, correlations_dis, x_batches, x_hat_batches, df_results = training_VAE(train_loader, test_loader, latent_dims, unique_symbols, num_epochs, learning_rate, test_size, kl_weight)\n",
    "best_correlation_dis = df_results['correlation_dis']\n",
    "best_correlation_cor = df_results['correlation_cor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder ECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import training_AE\n",
    "autoencoder_equations, train_losses, test_losses, correlations_cor, correlations_dis, x_batches, x_hat_batches, df_results = training_AE(train_loader, test_loader, latent_dims, unique_symbols, num_epochs, learning_rate, test_size)\n",
    "best_correlation_dis = df_results['correlation_dis']\n",
    "best_correlation_cor = df_results['correlation_cor']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_losses\n",
    "from src.models import AutoencoderEquations\n",
    "print(f\"correlation distance of last epoch: {best_correlation_dis}\")\n",
    "print(f\"correlation correlation of last epoch: {best_correlation_cor}\")\n",
    "print(AutoencoderEquations)\n",
    "\n",
    "if type(autoencoder_equations) == AutoencoderEquations:\n",
    "    df = None\n",
    "else:\n",
    "    df = df_results\n",
    "\n",
    "\n",
    "plot_losses(\n",
    "    train_losses,\n",
    "    test_losses,\n",
    "    correlation_cor=correlations_cor,\n",
    "    correlation_dis=correlations_dis,\n",
    "    df = df\n",
    ")\n",
    "last_correlations_cor = np.sum(correlations_cor[-10:]) / 10\n",
    "last_correlations_dis = np.sum(correlations_dis[-10:]) / 10\n",
    "print(f\"Last 10 epochs average correlation: {last_correlations_cor}\")\n",
    "print(f\"Last 10 epochs average correlation: {last_correlations_dis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.evaluation import evaluation_ec\n",
    "from src.evaluation import get_latent_representation\n",
    "from equation_tree.util.conversions import prefix_to_infix\n",
    "from src.evaluation import get_interpolated_df\n",
    "\n",
    "results, x_batches_p, x_hat_batches_p, x_constants_p, x_hat_constants_p = evaluation_ec(\n",
    "            kind='AE',\n",
    "            x_batches=x_batches,\n",
    "            x_hat_batches=x_hat_batches,\n",
    "            equation_tree_dataset=dataset,\n",
    "            max_len=max_len,)\n",
    "\n",
    "latent_space_representation, x_decoded, test_values, = get_latent_representation(\n",
    "    model=autoencoder_equations,\n",
    "    device=device,\n",
    "    test_dataloader=test_loader,\n",
    "    x_batches_p=x_batches_p,\n",
    "    x_hat_batches_p=x_hat_batches_p,\n",
    "    equation_tree_dataset=dataset,\n",
    ")\n",
    "\n",
    "# interpolated_df, z_list = get_interpolated_df(\n",
    "#     kind='AE',\n",
    "#     model=autoencoder_equations,\n",
    "#     equation_tree_dataset=dataset,\n",
    "#     latent_space_representation=latent_space_representation,\n",
    "#     equation_1=results[\"rand_idx1\"],\n",
    "#     equation_2=results[\"rand_idx2\"],\n",
    "#     c_1=float(results[\"original constants rand_idx1\"]),\n",
    "#     c_2=float(results[\"original constants rand_idx2\"]),\n",
    "#     num_interpolations = 5\n",
    "# )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.evaluation import get_correlation_coefficient\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "is_function = lambda x: x in [\"sin\", \"cos\", \"tan\", \"exp\", \"log\", \"sqrt\", \"abs\", \"acos\",\"asin\", \"arcsin\", \"arccos\"]\n",
    "is_operator = lambda x : x in [\"+\", \"-\", \"*\", \"/\", \"**\", \"max\", \"min\"]\n",
    "is_variable = lambda x : x in [\"x_1\"]\n",
    "is_constant = lambda x : x in [\"c_1\", \"c_0\"]\n",
    "\n",
    "correlation_cor, correlation_dis, distance_matrix_lat, distance_matrix_values, df, test_values_det, dm_values, distance_df_values = get_correlation_coefficient(\n",
    "    latent_space_representation=latent_space_representation, \n",
    "    x_decoded=x_decoded, \n",
    "    is_function=is_function, \n",
    "    is_operator=is_operator, \n",
    "    x_constants_p=x_constants_p, \n",
    "    test_values=test_values,\n",
    "    dataset=dataset,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow([[1, correlation_cor ], \n",
    "                 [correlation_cor, 1]], \n",
    "                 x = [\"Latent Space\", \"Function Correlation\"], \n",
    "                 y = [\"Latent Space\", \"Function Correlation\"], \n",
    "                 color_continuous_scale='RdBu', \n",
    "                 title=\"Correlation Matrix\")\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()\n",
    "print(f\"Correlation between Latent Space and Function correlation: {correlation_cor}\")\n",
    "fig = px.imshow([[1, correlation_dis ], \n",
    "                 [correlation_dis, 1]], \n",
    "                 x = [\"Latent Space\", \"Function Correlation\"], \n",
    "                 y = [\"Latent Space\", \"Function Correlation\"], \n",
    "                 color_continuous_scale='RdBu', \n",
    "                 title=\"Correlation Matrix\")\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()\n",
    "print(f\"Correlation between Latent Space and Function correlation: {correlation_dis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "l = 20\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=distance_matrix_lat[:l][:l],\n",
    "        x=df[\"Category\"][:l],\n",
    "        y=df[\"Category\"][:l],\n",
    "        colorscale=\"Viridis\",\n",
    "    ),\n",
    "    layout=go.Layout(\n",
    "        title=\"Latent Space distance matrix\",\n",
    "        xaxis=dict(title=\"Equation\"),\n",
    "        yaxis=dict(title=\"Equation\"),\n",
    "    ),\n",
    ")\n",
    "fig.update_layout(width=800, height=800)\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=distance_matrix_values[:l][:l],\n",
    "        x=df[\"Category\"][:l],\n",
    "        y=df[\"Category\"][:l],\n",
    "        colorscale=\"Viridis\",\n",
    "    ),\n",
    "    layout=go.Layout(\n",
    "        title=\"Values distance matrix\",\n",
    "        xaxis=dict(title=\"Equation\"),\n",
    "        yaxis=dict(title=\"Equation\"),\n",
    "    ),\n",
    ")\n",
    "# fig.update_traces(colorbar_dtick=\"log\")\n",
    "fig.update_layout(width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network appraoch (Currently not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank Values\n",
    "neighbours = np.zeros((len(distance_matrix_values)))\n",
    "dm = np.array(distance_matrix_values)\n",
    "# print(dm)\n",
    "# for i in range(len(distance_matrix_values)):\n",
    "\n",
    "# print(np.argmin(dm[i], axis=0))\n",
    "from collections import defaultdict\n",
    "\n",
    "graph = defaultdict(list)\n",
    "sorted = []\n",
    "for reference_point in range(len(distance_matrix_values)):\n",
    "    # Assuming the distance matrix is symmetric (same distance[i][j] = distance[j][i])\n",
    "    num_points = dm.shape[0]\n",
    "\n",
    "    # Create a list to store distances and indices\n",
    "    distances = [\n",
    "        (dm[reference_point][i], i) for i in range(num_points) if i != reference_point\n",
    "    ]\n",
    "\n",
    "    # Sort distances based on the first element (distance)\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Extract sorted indices\n",
    "    sorted_indices = [reference_point] + [index for _, index in distances]\n",
    "    graph[df[\"Category\"][reference_point]] = [\n",
    "        df[\"Category\"][i] for _, i in distances[:3]\n",
    "    ]\n",
    "    sorted.append(distances)\n",
    "\n",
    "\n",
    "# definition of function\n",
    "def addEdge(graph, u, v):\n",
    "    graph[u].append(v)\n",
    "\n",
    "\n",
    "edges = []\n",
    "\n",
    "\n",
    "def generate_edges(graph):\n",
    "    # for each node in graph\n",
    "    for node in graph:\n",
    "        # for each neighbour node of a single node\n",
    "        for neighbour in graph[node]:\n",
    "            # if edge exists then append\n",
    "            edges.append((node, neighbour))\n",
    "    return edges\n",
    "\n",
    "\n",
    "# for x in graph.keys():\n",
    "#   for y in graph[x]:\n",
    "#      addEdge(graph,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in range(len(distance_matrix_values)):\n",
    "    G.add_node(df[\"Category\"][i])\n",
    "\n",
    "edges = generate_edges(graph)\n",
    "for edge in edges:\n",
    "    index_node = df.index[df[\"Category\"] == edge[0]].tolist()[0]\n",
    "    index_neighbour = df.index[df[\"Category\"] == edge[1]].tolist()[0]\n",
    "    G.add_edge(\n",
    "        edge[0], edge[1], weight=distance_matrix_values[index_node][index_neighbour]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# TODO: for loop to add each node and its hovertext\n",
    "\n",
    "# Convert networkx graph to plotly figure\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "edge_x_start = []\n",
    "edge_y_start = []\n",
    "edge_x_end = []\n",
    "edge_y_end = []\n",
    "\n",
    "edges_names = []\n",
    "\n",
    "for edge in G.edges():\n",
    "    edges_names.append((edge[0], edge[1]))\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x_start.append(x0)\n",
    "    edge_y_start.append(y0)\n",
    "    edge_x_end.append(x1)\n",
    "    edge_y_end.append(y1)\n",
    "\n",
    "print(edges_names[0])\n",
    "edge_traces = []\n",
    "for i, (x_start, y_start, x_end, y_end) in enumerate(\n",
    "    zip(edge_x_start, edge_y_start, edge_x_end, edge_y_end)\n",
    "):\n",
    "    edge_trace = go.Scatter(\n",
    "        x=[x_start, x_end],\n",
    "        y=[y_start, y_end],\n",
    "        hoverinfo=\"text\",\n",
    "        name=edges_names[i][0],\n",
    "        mode=\"lines\",\n",
    "    )\n",
    "    edge_traces.append(edge_trace)\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_names = []\n",
    "\n",
    "for node in G.nodes():\n",
    "    node_names.append(node)\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "node_traces = []\n",
    "for i, (x, y) in enumerate(zip(node_x, node_y)):\n",
    "    node_name = node_names[i]\n",
    "    connections = graph[node_name]\n",
    "    real_distances = \"\"\n",
    "    lat_distances = \"\"\n",
    "    # get index of node in df['Category']\n",
    "    index_node = df.index[df[\"Category\"] == node_name].tolist()[0]\n",
    "\n",
    "    for connection in connections:\n",
    "        index_connection = df.index[df[\"Category\"] == connection].tolist()[0]\n",
    "        real_distances += f\"<br>Real Distance to {connection}: <b>{str(distance_matrix_values[index_node][index_connection])}</b>\"\n",
    "        lat_distances += f\"<br>Latent Space Distance to {connection}: {str(distance_matrix_lat[index_node][index_connection])}\"\n",
    "    node_trace = go.Scatter(\n",
    "        x=[x],\n",
    "        y=[y],\n",
    "        mode=\"markers+text\",\n",
    "        hoverinfo=\"text\",\n",
    "        hovertext=node_names[i] + real_distances + lat_distances,\n",
    "        textposition=\"bottom center\",\n",
    "        name=node_names[i],\n",
    "    )\n",
    "    node_traces.append(node_trace)\n",
    "\n",
    "data = edge_traces + node_traces\n",
    "fig = go.Figure(\n",
    "    data=data,\n",
    "    layout=go.Layout(\n",
    "        title=\"<br>Network graph made with Plotly\",\n",
    "        titlefont_size=16,\n",
    "        showlegend=True,\n",
    "        annotations=[\n",
    "            dict(showarrow=True, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.002)\n",
    "        ],\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest_path = G.shortest_path('asin((0.82*sin(abs(x_1))))', 'max(exp(x_1),x_1)')\n",
    "\n",
    "path = nx.all_pairs_shortest_path(G)\n",
    "path = dict(path)\n",
    "\n",
    "\n",
    "def nearest_neighbor_tsp(graph):\n",
    "    # Randomly pick a starting node\n",
    "    start_node = random.choice(list(graph.nodes))\n",
    "    current_node = start_node\n",
    "    unvisited_nodes = set(graph.nodes)\n",
    "    unvisited_nodes.remove(current_node)\n",
    "\n",
    "    # List to store the path\n",
    "    path = [current_node]\n",
    "\n",
    "    while unvisited_nodes:\n",
    "        # Find the nearest unvisited node\n",
    "        # print(graph[current_node])\n",
    "        # print([w for neighbours in graph[current_node].values() for w in neighbours.values()])\n",
    "        nearest_neighbor = np.argmin(\n",
    "            [\n",
    "                w\n",
    "                for neighbours in graph[current_node].values()\n",
    "                for w in neighbours.values()\n",
    "            ]\n",
    "        )\n",
    "        # Update current node and path\n",
    "        current_node = list(graph[current_node])[nearest_neighbor]\n",
    "        try:\n",
    "            unvisited_nodes.remove(current_node)\n",
    "        except KeyError:\n",
    "            print(\"Key not found\")\n",
    "        path.append(current_node)\n",
    "\n",
    "    # Add the starting node to complete the cycle\n",
    "    path.append(start_node)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = df[\"x\"][:5]\n",
    "y = df[\"y\"][:5]\n",
    "z = df[\"z\"][:5]\n",
    "\n",
    "# Calculate distances between points\n",
    "distances = []\n",
    "for i in range(len(x)):\n",
    "    row = []\n",
    "    for j in range(len(x)):\n",
    "        dist = np.sqrt((x[j] - x[i]) ** 2 + (y[j] - y[i]) ** 2 + (z[j] - z[i]) ** 2)\n",
    "        row.append(dist)\n",
    "    distances.append(row)\n",
    "\n",
    "# Create hover text\n",
    "hover_text = []\n",
    "for i in range(len(x)):\n",
    "    text = \"\"\n",
    "    for j in range(len(x)):\n",
    "        if i != j:\n",
    "            text += f\"Distance to {j}: {distances[i][j]:.2f}<br>\"\n",
    "    hover_text.append(text)\n",
    "\n",
    "data = []\n",
    "for i in range(5):\n",
    "    data_point = go.Scatter3d(\n",
    "        x=[x[i]],\n",
    "        y=[y[i]],\n",
    "        z=[z[i]],\n",
    "        hoverinfo=\"x+y+z+text\",\n",
    "        text=hover_text[i],\n",
    "        name=f\"{i}: {str(interpolated_df['Category'][i])}\",\n",
    "    )\n",
    "    data.append(data_point)\n",
    "\n",
    "# Create scatter plot\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    height=800,\n",
    "    title=\"Distance Between Points on Hover\",\n",
    "    xaxis=dict(title=\"X-axis\"),\n",
    "    yaxis=dict(title=\"Y-axis\"),\n",
    "    hovermode=\"closest\",\n",
    ")\n",
    "plot(\n",
    "    fig,\n",
    "    filename=\"distance.html\",\n",
    "    auto_open=False,\n",
    "    image=\"png\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import plot\n",
    "\n",
    "# TODO: show distances in hovertext\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    z=\"z\",\n",
    "    color=\"Category\",\n",
    "    hover_data=[\"Category\"],\n",
    "    title=\"Latent Space Representation\",\n",
    "    size_max=0.1,\n",
    ")\n",
    "plot(fig, filename=\"latent_space.html\", auto_open=False, image=\"png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results/ae_latent_space.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colour import Color\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(df, x=\"z\", y=\"y\",hue='Category', legend=False, palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vae = pd.read_csv(\"results/vae_latent_space.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df_vae, x=\"z\", y=\"y\",hue='Category', legend=False, palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import plot_latent_vectors\n",
    "\n",
    "# TODO: add distances to hovertext\n",
    "plot_latent_vectors(\n",
    "    z_list, interpolated_df, results, distance_matrix_lat, distance_matrix_values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.evaluation import plot_interpolations, generate_values\n",
    "import random\n",
    "\n",
    "for i in range(5):\n",
    "    rand_idx1 = random.randint(0, len(x_batches_p))\n",
    "    rand_idx2 = random.randint(0, len(x_batches_p))\n",
    "    df, _ = get_interpolated_df(\n",
    "        assignment = (is_function, is_operator, is_variable, is_constant),\n",
    "        kind=\"AE\",\n",
    "        model=autoencoder_equations,\n",
    "        equation_tree_dataset=dataset,\n",
    "        latent_space_representation=latent_space_representation,\n",
    "        equation_1=rand_idx1,\n",
    "        equation_2=rand_idx2,\n",
    "        c_1=1.5,\n",
    "        c_2=0.5,\n",
    "        num_interpolations=20,\n",
    "    )\n",
    "    if len(df.values)> 0:\n",
    "        plot_interpolations(df, (is_function, is_operator, is_variable, is_constant))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import plot_functions\n",
    "sin = 'sin(x_1*c_1)'\n",
    "cos = 'cos(x_1*c_1)'\n",
    "x = 'x_1'\n",
    "constants = [10,10,10]\n",
    "sin_values = generate_values(equation=sin, constant=constants[0], is_function=is_function, is_operator=is_operator, is_variable=is_variable, is_constant=is_constant)\n",
    "cos_values = generate_values(equation=cos, constant=constants[1], is_function=is_function, is_operator=is_operator, is_variable=is_variable, is_constant=is_constant)\n",
    "x_values = generate_values(equation=x, constant=constants[2], is_function=is_function, is_operator=is_operator, is_variable=is_variable, is_constant=is_constant)\n",
    "\n",
    "#print(f\"The two original functions ({sin} and ({cos})) have a correlation of {np.corrcoef(sin_values[1], cos_values[1])[0][1]}, a covariance of {np.cov(sin_values[1], cos_values[1])[0][1]} and a distance of {np.linalg.norm(np.array(sin_values[1]) - np.array(cos_values[1], ), ord=1)/25}\")\n",
    "#print(f\"The two original functions ({sin} and ({x})) have a correlation of {np.corrcoef(sin_values[1], x_values[1])[0][1]}, a covariance of {np.cov(sin_values[1], x_values[1])[0][1]} and a distance of {np.linalg.norm(np.array(sin_values[1]) - np.array(x_values[1], ), ord=1)/25}\")\n",
    "\n",
    "df_try ={\n",
    "    'functions': ['sin cos', 'x sin'],\n",
    "    'correlation': [np.corrcoef(sin_values[1], cos_values[1])[0][1], np.corrcoef(sin_values[1], x_values[1])[0][1]],\n",
    "    'covariance': [np.cov(sin_values[1], cos_values[1])[0][1], np.cov(sin_values[1], x_values[1])[0][1]],\n",
    "    'area between curves': [np.linalg.norm(np.array(sin_values[1]) - np.array(cos_values[1], ), ord=1)/25, np.linalg.norm(np.array(sin_values[1]) - np.array(x_values[1], ), ord=1)/25], \n",
    "    'cosine similarity': [np.dot(sin_values[1], cos_values[1])/(np.linalg.norm(sin_values[1])*np.linalg.norm(cos_values[1])), np.dot(sin_values[1], x_values[1])/(np.linalg.norm(sin_values[1])*np.linalg.norm(x_values[1]))]\n",
    "\n",
    "}\n",
    "df_try = pd.DataFrame(df_try)\n",
    "print(df_try)\n",
    "fig = plot_functions([['sin', '*', 'x_1', 'c_1'],['cos', '*', 'x_1', 'c_1'], ['x_1']], [sin_values, cos_values, x_values], constants=constants, is_function=is_function, is_operator=is_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equation_1 = 'x_1*c_1'\n",
    "equation_2 = 'x_1*c_1'\n",
    "df, _ = get_interpolated_df(\n",
    "    kind=\"AE\",\n",
    "    model=autoencoder_equations,\n",
    "    equation_tree_dataset=dataset,\n",
    "    latent_space_representation=latent_space_representation,\n",
    "    equation_1=equation_1,\n",
    "    equation_2=equation_2,\n",
    "    c_1=-1.0,\n",
    "    c_2=1.0,\n",
    "    num_interpolations=20,\n",
    "    assignment=(is_function, is_operator, is_variable, is_constant)\n",
    ")\n",
    "if len(df.values)> 0:\n",
    "    plot_interpolations(df, assignment=(is_function, is_operator, is_variable, is_constant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [[float(c)] for c in x_constants_p]\n",
    "fig = plot_functions(\n",
    "    equations=[results[\"x_decoded1\"], list(results[\"x_decoded2\"])],\n",
    "    constants=[cs[results[\"rand_idx1\"]], cs[results[\"rand_idx2\"]]],\n",
    "    values=[\n",
    "        test_values_det[results[\"rand_idx1\"]],\n",
    "        test_values_det[results[\"rand_idx2\"]],\n",
    "    ],\n",
    "    is_function=is_function,\n",
    "    is_operator=is_operator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import random_embedding\n",
    "\n",
    "random_embedding('AE', autoencoder_equations, dataset, latent_dims, (is_function, is_operator, is_variable, is_constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse number of latent units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.evaluation import evaluate_different_models\n",
    "\n",
    "datasets = [torch.load(\"./data/dataset1.pt\"), torch.load(\"./data/dataset2.pt\"), torch.load(\"./data/dataset3.pt\"),torch.load(\"./data/dataset4.pt\"),torch.load(\"./data/dataset5.pt\"), torch.load(\"./data/dataset6.pt\"),torch.load(\"./data/dataset7.pt\"), torch.load(\"./data/dataset8.pt\"), torch.load(\"./data/dataset9.pt\"),torch.load(\"./data/dataset10.pt\")]\n",
    "\n",
    "\n",
    "df_units = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants'])\n",
    "count = 0\n",
    "for d in datasets: \n",
    "    count += 1\n",
    "    for units in [2,4,8,16,32,64,128]: \n",
    "        print(f\"Dataset {count} with {units} units\")\n",
    "        dct = evaluate_different_models(d, batch_size, training_set_proportion, units, num_epochs, learning_rate, 'AE', 0.0001, classes=None, assignments=(is_function, is_operator, is_variable, is_constant))\n",
    "        df = pd.DataFrame(dct, index=[0])\n",
    "        df_units = pd.concat([df_units, df], ignore_index=True, axis=0)\n",
    "df_units.to_csv('results/latent_dims_ae_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Kl divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_different_models\n",
    "datasets = [torch.load(\"./data/dataset1.pt\"), torch.load(\"./data/dataset2.pt\"), torch.load(\"./data/dataset3.pt\"),torch.load(\"./data/dataset4.pt\"),torch.load(\"./data/dataset5.pt\"), torch.load(\"./data/dataset6.pt\"),torch.load(\"./data/dataset7.pt\"), torch.load(\"./data/dataset8.pt\"), torch.load(\"./data/dataset9.pt\"),torch.load(\"./data/dataset10.pt\")]\n",
    "\n",
    "is_VAE = True\n",
    "if is_VAE:\n",
    "    df_kl = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants', 'kl_weight', 'test_reconstruction_loss', 'test_constant_loss', 'test_latent_correlation_loss', 'test_kl_divergence', 'correlation_dis reonstructed equations'])\n",
    "else:\n",
    "    df_kl = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants'])\n",
    "\n",
    "\n",
    "count = 0\n",
    "for d in datasets: \n",
    "    count += 1\n",
    "    for kl_weight in [0, 0.0001, 0.001, 0.01, 0.1, 1]: \n",
    "        print(f\"Dataset {count} with {kl_weight} kl_weight\")\n",
    "        dct = evaluate_different_models(d, batch_size, training_set_proportion, latent_dims, num_epochs, learning_rate, is_VAE, kl_weight)\n",
    "        df = pd.DataFrame(dct, index=[0])\n",
    "        df_kl = pd.concat([df_kl, df], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_different_models\n",
    "datasets = [torch.load(\"./data/dataset1.pt\"), torch.load(\"./data/dataset2.pt\"), torch.load(\"./data/dataset3.pt\"),torch.load(\"./data/dataset4.pt\"),torch.load(\"./data/dataset5.pt\"), torch.load(\"./data/dataset6.pt\"),torch.load(\"./data/dataset7.pt\"), torch.load(\"./data/dataset8.pt\"), torch.load(\"./data/dataset9.pt\"),torch.load(\"./data/dataset10.pt\")]\n",
    "\n",
    "is_VAE = False\n",
    "if is_VAE:\n",
    "    df_lr = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants', 'kl_weight', 'test_reconstruction_loss', 'test_constant_loss', 'test_latent_correlation_loss', 'test_kl_divergence', 'correlation_dis reonstructed equations', 'learning_rate'])\n",
    "else:\n",
    "    df_lr = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants', 'learning_rate'])\n",
    "\n",
    "\n",
    "count = 0\n",
    "for d in datasets: \n",
    "    count += 1\n",
    "    for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]: \n",
    "        print(f\"Dataset {count} with {lr} learning rate\")\n",
    "        dct = evaluate_different_models(d, batch_size, training_set_proportion, latent_dims, num_epochs, lr, is_VAE, 0.001)\n",
    "        df = pd.DataFrame(dct, index=[0])\n",
    "        df_lr = pd.concat([df_lr, df], ignore_index=True, axis=0)\n",
    "df_lr.to_csv('results/learning_rate_ae_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
