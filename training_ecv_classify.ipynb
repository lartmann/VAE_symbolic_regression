{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82adac62",
   "metadata": {},
   "source": [
    "# Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import plotly.io as pio\n",
    "import copy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 30\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# DATA PARAMETERS\n",
    "num_evaluation_samples = 50                                       \n",
    "inf_repacement = 1000\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "training_set_proportion = 0.8                         \n",
    "num_epochs = 500                                    \n",
    "batch_size = 50          \n",
    "learning_rate = 0.001\n",
    "kl_weight = 0.0001\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "latent_dims = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Requirements:\n",
    "- one random constant\n",
    "- one generate equation\n",
    "- values of the instantiated equation\n",
    "\n",
    "![](./images/dataloader.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.preprocessing import generate_dataset, preprocessing\n",
    "from src.evaluation import generate_values\n",
    "import json\n",
    "import equation_tree\n",
    "from equation_tree.util.conversions import infix_to_prefix, prefix_to_infix\n",
    "from src.preprocessing import EquationDatasetClassify\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from equation_tree.tree import node_from_prefix\n",
    "\n",
    "is_function = lambda x: x in [\"sin\", \"exp\"]\n",
    "is_operator = lambda x : x in [\"+\", \"-\", \"*\"]\n",
    "is_variable = lambda x : x in [\"x_1\", \"x_1\", \"X\"]\n",
    "is_constant = lambda x : x in [\"c_0\"]\n",
    "max_len = 0\n",
    "# read json file \n",
    "classes = json.load(open('second_order_simple.json', 'r'))\n",
    "\n",
    "# transfrom to prefix notation\n",
    "classes_list = []\n",
    "global classes_dict\n",
    "classes_dict = {}\n",
    "for cl in classes:\n",
    "    try:\n",
    "        eq_prefix = infix_to_prefix(cl.replace('X', 'x_1', ), is_function, is_operator)\n",
    "        inf_eq = prefix_to_infix(eq_prefix, is_function, is_operator)\n",
    "        classes_list.append(eq_prefix)\n",
    "        classes_dict[cl] = eq_prefix\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "equations = []\n",
    "values = []\n",
    "constants = []\n",
    "\n",
    "# generate dataset\n",
    "for e, cl in enumerate(classes_list):\n",
    "    for i in range(100):\n",
    "        c = random.random()*10\n",
    "        v = generate_values(cl, c, is_function, is_operator, is_variable, is_constant, infix=classes[e])\n",
    "        if len(cl) > max_len:\n",
    "             max_len = len(cl)\n",
    "        if v != (None,):\n",
    "            equations.append(cl)\n",
    "            values.append(v)\n",
    "            constants.append([c])\n",
    "\n",
    "equations_final = []\n",
    "for eq_prefix in equations:\n",
    "    # try block due to complex infinity exception\n",
    "    # add padding so that all equations have the same shape\n",
    "    if len(eq_prefix) < max_len:\n",
    "        eq_prefix = eq_prefix + [\"<PAD>\"] * (max_len - len(eq_prefix))\n",
    "    # add equations, constants and values to their list\n",
    "    equations_final.append(eq_prefix)\n",
    "\n",
    "\n",
    "all_symbols = [item for sublist in equations_final for item in sublist]\n",
    "unique_symbols = sorted(list(set(all_symbols)))\n",
    "\n",
    "# obtain mapping from symbols to indices and vice versa\n",
    "symb_to_idx = {symbol: idx for idx, symbol in enumerate(unique_symbols)}\n",
    "idx_to_symb = {idx: symb for symb, idx in symb_to_idx.items()}\n",
    "\n",
    "dataset = EquationDatasetClassify(equations_final, values, symb_to_idx,len(unique_symbols), constants)\n",
    "\n",
    "classes = set(tuple(i) for i in equations_final)\n",
    "classes = [list(i) for i in classes]\n",
    "classes = [dataset.encode_equation(i) for i in classes]\n",
    "print(len(classes), len(dataset))\n",
    "\n",
    "train_loader, test_loader, test_size = preprocessing(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    training_set_proportion=training_set_proportion\n",
    ")\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.evaluation import plot_functions\n",
    "\n",
    "# visualize 20 first equations in the dataset\n",
    "plot_functions(\n",
    "    equations=equations[:20],\n",
    "    constants=constants[:20],\n",
    "    values=values[:20],\n",
    "    is_function=is_function,\n",
    "    is_operator=is_operator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import training_AE_C\n",
    "autoencoder_equations, train_losses, test_losses, correlations_cor, correlations_dis, correlations_dis_train, x_batches, x_hat_batches, df_results = training_AE_C(train_loader, test_loader, latent_dims, unique_symbols, num_epochs, learning_rate, test_size, max_len, classes, 1.0)\n",
    "best_correlation_dis = df_results['correlation_dis']\n",
    "best_correlation_cor = df_results['correlation_cor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import training_VAE_C\n",
    "autoencoder_equations, train_losses, test_losses, correlations_cor, correlations_dis, x_batches, x_hat_batches, df_results = training_VAE_C(train_loader, test_loader, latent_dims, unique_symbols, num_epochs, learning_rate, test_size, kl_weight, classes, max_len, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.utils import plot_losses\n",
    "\n",
    "plot_losses(\n",
    "    train_losses,\n",
    "    test_losses,\n",
    "    correlation_cor=correlations_cor,\n",
    "    correlation_dis=correlations_dis,\n",
    "    df = df_results,\n",
    ")\n",
    "last_correlations_cor = np.sum(correlations_cor[-10:]) / 10\n",
    "last_correlations_dis = np.sum(correlations_dis[-10:]) / 10\n",
    "print(f\"Last 10 epochs average correlation: {last_correlations_cor}\")\n",
    "print(f\"Last 10 epochs average correlation: {last_correlations_dis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.evaluation import evaluation_ec\n",
    "from src.evaluation import get_latent_representation\n",
    "from equation_tree.util.conversions import prefix_to_infix\n",
    "from src.evaluation import get_interpolated_df\n",
    "\n",
    "\n",
    "x_hat_batches_n = [torch.argmax(batch[0], dim=1).tolist() for batch in x_hat_batches]\n",
    "# constants\n",
    "\n",
    "for i, batch in enumerate(x_hat_batches_n):\n",
    "    for j, eq in enumerate(batch):\n",
    "        x_hat_batches_n[i][j] = classes[eq]\n",
    "\n",
    "# concatenate all batches\n",
    "x_hat_batches_n = [item for sublist in x_hat_batches_n for item in sublist]\n",
    "x_batches_n = [item for sublist in x_batches for item in sublist[0]]\n",
    "x_constants = [item for sublist in x_batches for item in sublist[1]]\n",
    "# caclulate accuracy\n",
    "count = 0\n",
    "for rec, real in zip(x_hat_batches_n, x_batches_n):\n",
    "    if rec == real.tolist():\n",
    "        count += 1\n",
    "    else: \n",
    "        print(f\"rec: {rec}, real: {real}\")\n",
    "\n",
    "accuracy = count / (len(x_hat_batches_n))\n",
    "print(f\"Equation reconstruction accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.imshow([[1, correlations_dis[-1] ], \n",
    "                 [correlations_dis[-1], 1]], \n",
    "                 x = [\"Latent Space\", \"Function Correlation\"], \n",
    "                 y = [\"Latent Space\", \"Function Correlation\"], \n",
    "                 color_continuous_scale='RdBu', \n",
    "                 title=\"Correlation Matrix\")\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()\n",
    "print(f\"Correlation between Latent Space and Function correlation: {correlations_dis[-1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_space_representation, x_decoded, test_values, = get_latent_representation(\n",
    "    model=autoencoder_equations,\n",
    "    device=device,\n",
    "    test_dataloader=test_loader,\n",
    "    x_batches_p=x_batches_n,\n",
    "    x_hat_batches_p=x_hat_batches_n,\n",
    "    equation_tree_dataset=dataset,\n",
    "    num_interpolations=5\n",
    ")\n",
    "len(latent_space_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Latent Space\n",
    "Plot the first 3 latent space dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import plot\n",
    "import plotly.express as px\n",
    "from equation_tree.util.conversions import prefix_to_infix\n",
    "\n",
    "df = {\n",
    "    \"x\": latent_space_representation[:, 0],\n",
    "    \"y\": latent_space_representation[:, 1],\n",
    "    \"z\": latent_space_representation[:, 2],\n",
    "    \"category\": [prefix_to_infix(dataset.decode_equation(eq.tolist()), is_function, is_operator).replace(\"c_0\", str(round(float(const[0]),2))) for eq, const in zip(x_decoded, x_constants)], #TODO: replace constant wit real value\n",
    "}\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    z=\"z\",\n",
    "    color=\"category\",\n",
    "    title=\"Latent Space Representation\",\n",
    "    size_max=0.1,\n",
    ")\n",
    "plot(fig, filename=\"latent_space.html\", auto_open=False, image=\"png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colour import Color\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# plots the first 2 dimensions of the latent space\n",
    "sns.scatterplot(df, x=\"x\", y=\"y\",hue='category', legend=False, palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import plot_interpolations\n",
    "equation_1 = 'x_1*c_1'\n",
    "equation_2 = 'x_1+c_1'\n",
    "df, _ = get_interpolated_df(\n",
    "    kind=\"classifier\",\n",
    "    model=autoencoder_equations,\n",
    "    equation_tree_dataset=dataset,\n",
    "    latent_space_representation=latent_space_representation,\n",
    "    equation_1=equation_1,\n",
    "    equation_2=equation_2,\n",
    "    c_1=-1.0,\n",
    "    c_2=1.0,\n",
    "    num_interpolations=20,\n",
    "    assignment=(is_function, is_operator, is_variable, is_constant),\n",
    "    classes=classes,\n",
    ")\n",
    "if len(df.values)> 0:\n",
    "    fig = plot_interpolations(df, assignment=(is_function, is_operator, is_variable, is_constant))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from src.evaluation import plot_interpolations, generate_values\n",
    "\n",
    "# plot the interpolation between two functions\n",
    "for i in range(1):\n",
    "    rand_idx1 = random.randint(0, len(x_batches_n))\n",
    "    rand_idx2 = random.randint(0, len(x_batches_n))\n",
    "    df, _ = get_interpolated_df(\n",
    "        kind=\"classifier\",\n",
    "        model=autoencoder_equations,\n",
    "        equation_tree_dataset=dataset,\n",
    "        latent_space_representation=latent_space_representation,\n",
    "        equation_1=rand_idx1,\n",
    "        equation_2=rand_idx2,\n",
    "        c_1=1.5,\n",
    "        c_2=0.5,\n",
    "        num_interpolations=20,\n",
    "        assignment=(is_function, is_operator, is_variable, is_constant),\n",
    "        classes=classes,\n",
    "    )\n",
    "    if len(df.values)> 0:\n",
    "        fig = plot_interpolations(df, assignment=(is_function, is_operator, is_variable, is_constant))\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import random_embedding\n",
    "\n",
    "# evaluate how many of the random embeddings returned valid functions\n",
    "random_embedding('AE_C', autoencoder_equations, dataset, latent_dims, (is_function, is_operator, is_variable, is_constant), classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sytematic Evaluation\n",
    "Every parameter change is evaluated 10 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse number of latent units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from src.evaluation import evaluate_different_models\n",
    "\n",
    "datasets = [dataset] * 10\n",
    "\n",
    "kind = \"VAE_C\"\n",
    "weight = 1.0\n",
    "kl_weight = 0.0001\n",
    "\n",
    "df_units = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants'])\n",
    "count = 0\n",
    "for d in datasets: \n",
    "    count += 1\n",
    "    for units in [32, 64, 128]: \n",
    "        print(f\"Dataset {count} with {units} units\")\n",
    "        dct = evaluate_different_models(d, batch_size, training_set_proportion, units, num_epochs, learning_rate, kind, weight, classes=classes, assignments=(is_function, is_operator, is_variable, is_constant), klweight=kl_weight)\n",
    "        print(f\"Correlation: {dct['correlation_dis']}, Accuracy: {dct['accuracy equations']}\")\n",
    "        df = pd.DataFrame(dct, index=[0])\n",
    "        df_units = pd.concat([df_units, df], ignore_index=True, axis=0)\n",
    "df_units.to_csv('results/latent_dims_vae_calssify_big_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse KL weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_different_models\n",
    "datasets = [dataset] * 10\n",
    "kind = \"VAE_C\"\n",
    "\n",
    "#df_weight = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individual)', 'accuracy equations', 'constant MSE', 'average distance constants', 'weight', 'kl_weight', 'test_reconstruction_loss', 'test_constant_loss', 'test_latent_correlation_loss', 'test_kl_divergence', 'correlations_dis_train'])\n",
    "\n",
    "\n",
    "count = 0\n",
    "for d in datasets: \n",
    "    count += 1\n",
    "    for klweight in [0.00001]: \n",
    "        print(f\"Dataset {count} with a weighting of {klweight} for the latent correlation loss\")\n",
    "        dct = evaluate_different_models(d, batch_size, training_set_proportion, latent_dims, num_epochs, learning_rate, kind, 1.0, klweight=klweight, classes=classes, assignments=(is_function, is_operator, is_variable, is_constant))\n",
    "        #print(dct)\n",
    "        print(f\"Correlation: {dct['correlation_dis']}, Accuracy: {dct['accuracy equations']}\")\n",
    "        df = pd.DataFrame(dct, index=[0])\n",
    "        df_weight = pd.concat([df_weight, df], ignore_index=True, axis=0)\n",
    "df_weight.to_csv('results/classify_klweight_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_different_models\n",
    "datasets = [dataset] * 10\n",
    "is_VAE = True\n",
    "if is_VAE:\n",
    "    df_lr = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants', 'kl_weight', 'test_reconstruction_loss', 'test_constant_loss', 'test_latent_correlation_loss', 'test_kl_divergence', 'correlation_dis reonstructed equations', 'learning_rate'])\n",
    "else:\n",
    "    df_lr = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individualt)', 'accuracy equations', 'constant MSE', 'average distance constants', 'learning_rate'])\n",
    "\n",
    "kind = 'VAE_C'\n",
    "count = 0\n",
    "for d in datasets: \n",
    "    count += 1\n",
    "    for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]: \n",
    "        print(f\"Dataset {count} with {lr} learning rate\")\n",
    "        dct = evaluate_different_models(d, batch_size, training_set_proportion, latent_dims, num_epochs, lr, kind, 1.0, klweight=klweight, classes=classes, assignments=(is_function, is_operator, is_variable, is_constant))\n",
    "        df = pd.DataFrame(dct, index=[0])\n",
    "        print(f\"Correlation: {dct['correlation_dis']}, Accuracy: {dct['accuracy equations']}\")\n",
    "        df_lr = pd.concat([df_lr, df], ignore_index=True, axis=0)\n",
    "df_lr.to_csv('results/learning_rate_vae_classify.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Correlation weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_different_models\n",
    "datasets = [dataset] * 10\n",
    "kind = \"VAE_C\"\n",
    "\n",
    "df_weight = pd.DataFrame(columns=['latent dims', 'correlation_cor', 'correlation_dis', 'correlation_cor last 10 epochs', 'correlation_dis last 10 epochs', 'recovered equations', 'accuracy (individual)', 'accuracy equations', 'constant MSE', 'average distance constants', 'weight', 'kl_weight', 'test_reconstruction_loss', 'test_constant_loss', 'test_latent_correlation_loss', 'test_kl_divergence', 'correlations_dis_train'])\n",
    "\n",
    "\n",
    "count = 0\n",
    "for d in datasets: \n",
    "    count += 1\n",
    "    for weight in [0, 0.1, 1, 10, 100, 1000]: \n",
    "        print(f\"Dataset {count} with a weighting of {weight} for the latent correlation loss\")\n",
    "        dct = evaluate_different_models(d, batch_size, training_set_proportion, latent_dims, num_epochs, learning_rate, kind, weight, klweight=kl_weight, classes=classes, assignments=(is_function, is_operator, is_variable, is_constant))\n",
    "        #print(dct)\n",
    "        print(f\"Correlation: {dct['correlation_dis']}, Accuracy: {dct['accuracy equations']}\")\n",
    "        df = pd.DataFrame(dct, index=[0])\n",
    "        df_weight = pd.concat([df_weight, df], ignore_index=True, axis=0)\n",
    "df_weight.to_csv('results/classify_weight_vae.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from src.evaluation import generate_values, decode_latent_classify\n",
    "#target_function = \"(c_0-x_1)*exp(x_1)\"\n",
    "target_function = \"exp(x_1)*c_0\"\n",
    "#target_function = \"x_1+exp(x_1)+c_0\"\n",
    "target_function = \"c_0*x_1\"\n",
    "num_samples = 1000\n",
    "target_constant = 5.0\n",
    "target_dist = generate_values(target_function, target_constant, is_function, is_operator, is_variable, is_constant)\n",
    "iteration_values = []\n",
    "\n",
    "# define the probabilistic model\n",
    "def probabilistic_model(data):\n",
    "    latent_variables = []\n",
    "    for i in range(latent_dims):\n",
    "        latent_variables.append(pyro.sample(f\"latent_variable_{i}\", dist.Normal(0, 5)))\n",
    "    variance = torch.tensor(0.1) * 50\n",
    "\n",
    "    embedding = [latent_variables]\n",
    "\n",
    "    equations, constants = decode_latent_classify(autoencoder_equations, dataset, embedding, classes)\n",
    "    values = generate_values(equations[0], constants[0][0][0], is_function, is_operator, is_variable, is_constant)[1]\n",
    "    try: \n",
    "        values = torch.tensor(values, dtype=torch.float32)\n",
    "    except:\n",
    "        print(values)\n",
    "        print(type(values))\n",
    "    \n",
    "    pyro.sample(f\"observed_data\", dist.Normal(values, variance).to_event(1), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import pyro.poutine as poutine\n",
    "import torch\n",
    "from pyro.infer import MCMC, NUTS\n",
    "pyro.clear_param_store()\n",
    "\n",
    "\n",
    "observed_data = torch.tensor(target_dist[1])\n",
    "\n",
    "nuts_kernel = NUTS(probabilistic_model)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=200)\n",
    "\n",
    "mcmc.run(torch.tensor(target_dist[1]))\n",
    "\n",
    "samples = mcmc.get_samples()\n",
    "print(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# take just the mean of the samples\n",
    "for i in range(latent_dims):\n",
    "    results.append(samples[f'latent_variable_{i}'].mean())\n",
    "\n",
    "# sample from the distribution\n",
    "results_sampled = []\n",
    "#for i in range(latent_dims):\n",
    "    #results_sampled.append(dist.Normal(samples[f'latent_variable_{i}'].mean(), samples[f'latent_variable_{i}'].std()).sample())\n",
    "\n",
    "# random embedding\n",
    "random_samples = []\n",
    "for i in range(latent_dims):\n",
    "    random_samples.append(dist.Normal(0, 5).sample())\n",
    "\n",
    "# decode results\n",
    "results_dec = decode_latent_classify(autoencoder_equations, dataset, [results], classes)\n",
    "result_equation = results_dec[0][0]\n",
    "result_constant = results_dec[1][0][0][0]\n",
    "\n",
    "# decode random samples\n",
    "random_dec = decode_latent_classify(autoencoder_equations, dataset, [random_samples], classes)\n",
    "random_equation = random_dec[0][0]\n",
    "random_constant = random_dec[1][0][0][0]\n",
    "\n",
    "print(f\"resulting function: {prefix_to_infix(result_equation, is_function, is_operator)} with constant {result_constant}\")\n",
    "print(observed_data)\n",
    "\n",
    "v_sample = generate_values(result_equation, result_constant, is_function, is_operator, is_variable, is_constant)\n",
    "v_real = generate_values(target_function, target_constant, is_function, is_operator, is_variable, is_constant)\n",
    "v_random = generate_values(random_equation, random_constant, is_function, is_operator, is_variable, is_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_sampling_functions(smpls, with_Random):\n",
    "    # latent variabls values\n",
    "    latent_variable_MCMC = []\n",
    "    equations_MCMC = []\n",
    "    constants_MCMC = []\n",
    "    # generate a list of lists for the latent variables of each iteration (num_iterations, latent_dims)\n",
    "    for s in range(len(smpls['latent_variable_0'])):\n",
    "        embedding = []\n",
    "        for i in range(latent_dims):\n",
    "            embedding.append(smpls[f'latent_variable_{i}'][s].item())\n",
    "        equations, constants = decode_latent_classify(autoencoder_equations, dataset, [embedding], classes)\n",
    "        values = torch.tensor(generate_values(equations[0], constants[0][0][0], is_function, is_operator, is_variable, is_constant)[1], dtype=torch.float32)\n",
    "        latent_variable_MCMC += values.detach().numpy().tolist()\n",
    "        constants_MCMC += [constants[0][0][0]]\n",
    "        equations_MCMC += [prefix_to_infix(equations[0], is_function, is_operator)]*50\n",
    "\n",
    "    df = {\n",
    "        \"y\": latent_variable_MCMC,\n",
    "        \"x\": np.linspace(-1,1,50).tolist() * num_samples,\n",
    "        \"equation\": equations_MCMC\n",
    "\n",
    "    }\n",
    "\n",
    "    df_compare = {\n",
    "        'x': np.linspace(-1, 1, 50),\n",
    "        'y_sample': v_sample[1],\n",
    "        'y_real': v_real[1],\n",
    "        'y_random': v_random[1]\n",
    "    }   \n",
    "    print(f\"the smallest constant is {min(constants_MCMC)} and the largest constant is {max(constants_MCMC)}\")\n",
    "    data = pd.DataFrame(df)\n",
    "    #g = sns.lineplot(data=data, x='x', y='y', hue='equation', fit_reg=True, legend=False, height=5, scatter_kws={'alpha':0.5, 's': 0.05})\n",
    "    sns.lineplot(data=data, x='x', y='y', hue='equation')\n",
    "    #sns.lineplot(data=df_compare, x='x', y='y_sample', label=f\"Sampled: {prefix_to_infix(result_equation, is_function, is_operator).replace('c_0', str(round(result_constant, 2)) )}\")\n",
    "    sns.lineplot(data=df_compare, x='x', y='y_real', label=f\"Real: {target_function.replace('c_0', str(target_constant))}\")\n",
    "    if with_Random:\n",
    "        sns.lineplot(data=df_compare, x='x', y='y_random', label=f\"Random: {prefix_to_infix(random_equation, is_function, is_operator).replace('c_0', str(round(random_constant, 2)) )}\")\n",
    "\n",
    "    plt.legend(markerscale=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_dist(smpls):\n",
    "    # Plot resulting probability distribution\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(latent_dims):\n",
    "        sns.histplot(smpls[f'latent_variable_{i}'], kde=True, ax=ax)\n",
    "\n",
    "    # add legend\n",
    "    ax.legend([f\"latent_variable_{i}\" for i in range(latent_dims)])\n",
    "\n",
    "    ax.set_title(\"Posterior distribution of the latent variables\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampling_functions(samples, False)\n",
    "plot_dist(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
